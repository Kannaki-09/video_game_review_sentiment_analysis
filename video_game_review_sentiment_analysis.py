# -*- coding: utf-8 -*-
"""PA_Assignment_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gVpxBV82XIX4K9RUSSLgopNUch5fbIEy
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
import gzip
import pickle
import re
import sys
import string
import nltk
import html
# %matplotlib inline
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from nltk.stem.porter import PorterStemmer
porter_stemmer = PorterStemmer()
from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
from nltk.probability import FreqDist
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import wordnet
from nltk import pos_tag
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_treebank_pos_tagger')
from sklearn import metrics
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_curve, auc
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from matplotlib import cm
from wordcloud import WordCloud
from unicodedata import normalize
from ast import literal_eval
sns.set()

def parse(path):
  g = gzip.open(path, 'rb')
  for l in g:
    yield json.loads(l)

def getDF(path):
  i = 0
  df = {}
  for d in parse(path):
    df[i] = d
    i += 1
  return pd.DataFrame.from_dict(df, orient='index')

df = getDF('/content/drive/MyDrive/Colab Notebooks/reviews_Video_Games.json.gz')

df.head()

df_new = df[:100000]

df_new.head()

df_new.shape

df_new = df_new[['overall', 'reviewerID', 'asin', 'reviewText', 'summary', 'unixReviewTime']]

df_new.head()

df_new = df_new.drop_duplicates()
df_new.dropna(inplace=True)

df_asin_mean = df_new.groupby(['asin']).mean()
# find out the median of ratings
df_asin_mean['overall'].median()
# plot boxplot & histogram
sns.set(style="darkgrid")
f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={"height_ratios": (.15, .85)})
sns.boxplot(df_asin_mean['overall'], ax=ax_box)
sns.histplot(df_asin_mean['overall'], ax=ax_hist)
ax_box.set(xlabel='')
plt.axvline(x =df_asin_mean['overall'].median() ,color = "red",ls='--',lw =2)
plt.show()

# create a variable showing the length of each review
df_new['reviews_length']=df_new['reviewText'].apply(len)
df2 = df_new.groupby(df_new['overall']).mean()

df2

# plot bar chart
x=df2.iloc[:,1]
x = x.values[::-1]
y=df2.index[::-1]
ax = sns.barplot(x=y, y=x)
ax.set(xlabel='rating counts', ylabel='score')
plt.show()

# Code to append reviews based on their rating
five_star_reviews = df_new.loc[df['overall'] == 5, 'reviewText']
four_star_reviews = df_new.loc[df['overall'] == 4, 'reviewText']
three_star_reviews = df_new.loc[df['overall'] == 3, 'reviewText']
two_star_reviews = df_new.loc[df['overall'] == 2, 'reviewText']
one_star_reviews = df_new.loc[df['overall'] == 1, 'reviewText']
below_four = three_star_reviews.append(two_star_reviews).append(one_star_reviews)
four_and_five = five_star_reviews.append(four_star_reviews)

# Concatenate all reviews in 'below_four' into a single string
below_four_text = " ".join(review for review in below_four)

# Prepare stopwords
stopwords_set = set(stopwords.words('english'))

# Generate a word cloud image for reviews with less than 4 stars
wordcloud = WordCloud(stopwords=stopwords_set, background_color="white").generate(below_four_text)

# Display the generated image using matplotlib
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# plot wordcloud for reviews with 4 stars or above
# Concatenate all reviews in 'four_and_five' into a single string
four_and_five_text = " ".join(review for review in four_and_five)

# Prepare stopwords, assuming you have them set up correctly as shown previously
stopwords_set = set(stopwords.words('english'))

# Generate a word cloud image
wordcloud = WordCloud(stopwords=stopwords_set, background_color="white").generate(four_and_five_text)
# Display the generated image:
# the matplotlib way:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

df_new['target']=[0  if x<=3 else 1 for x in df_new['overall']]
df_new['combined_text'] = df_new['reviewText'] + ' ' + df_new['summary']

# bigrams
def get_top_n_bigram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

common_words_good = get_top_n_bigram(df_new[df_new['target']==1]['combined_text'], 30)
common_words_bad= get_top_n_bigram(df_new[df_new['target']==0]['combined_text'], 30)

plt.figure(figsize=(25,8))
# good reviews bigrams
plt.subplot(1,2,1)
x_good=[x[0] for x in common_words_good]
y_good=[x[1] for x in common_words_good]
sns.barplot(x=x_good, y=y_good,color='g')
plt.xticks(rotation=90,fontsize=20)
plt.title("Top 20 Bigrams in Positive Reviews",fontsize=20)
for i in range(len(x_good)):
    plt.text(i-0.2,y_good[i]+100,'{}'.format(y_good[i]),size=15,rotation=45)
# badreviews bigrams
plt.subplot(1,2,2)
x_bad=[x[0] for x in common_words_bad]
y_bad=[x[1] for x in common_words_bad]
sns.barplot(x=x_bad,y=y_bad,color='b')
plt.xticks(rotation=90,fontsize=20)
plt.title("Top 20 Bigrams in Negative Reviews",fontsize=20)
for i in range(len(x_bad)):
    plt.text(i-0.2,y_bad[i],'{}'.format(y_bad[i]),size=15,rotation=45)

#Delete Special Characters
pattern = r"\&\#[0-9]+\;"
df_new["preprocessed_text"] = df_new["combined_text"].str.replace(pat=pattern, repl="", regex=True)

df_new["preprocessed_text"][0]

# reduce words to root form - using Lemmatization to reduce tokens to their base word
def lemmatize_word(tagged_token):
    """ Returns lemmatized word given its tag"""
    root = []
    for token in tagged_token:
        tag = token[1][0]
        word = token[0]
        if tag.startswith('J'):
            root.append(wordnet_lemmatizer.lemmatize(word, wordnet.ADJ))
        elif tag.startswith('V'):
            root.append(wordnet_lemmatizer.lemmatize(word, wordnet.VERB))
        elif tag.startswith('N'):
            root.append(wordnet_lemmatizer.lemmatize(word, wordnet.NOUN))
        elif tag.startswith('R'):
            root.append(wordnet_lemmatizer.lemmatize(word, wordnet.ADV))
        else:
            root.append(word)
    return root
def lemmatize_doc(document):
    """ Tags words then returns sentence with lemmatized words"""
    lemmatized_list = []
    tokenized_sent = sent_tokenize(document)
    for sentence in tokenized_sent:
        no_punctuation = re.sub(r"[`'\",.!?()]", " ", sentence)
        tokenized_word = word_tokenize(no_punctuation)
        tagged_token = pos_tag(tokenized_word)
        lemmatized = lemmatize_word(tagged_token)
        lemmatized_list.extend(lemmatized)
    return " ".join(lemmatized_list)

# use functions defined above
df_new["preprocessed_text"] = df_new["preprocessed_text"].apply(lambda row: lemmatize_doc(row))

#Remove Accents
remove_accent = lambda text: normalize("NFKD", text).encode("ascii", "ignore").decode("utf-8", "ignore")
df_new["accent_text"] = df_new["preprocessed_text"].apply(remove_accent)

def tokenization(text):
    tokens = nltk.word_tokenize(text)
    return tokens
    stopwords = nltk.corpus.stopwords.words('english')

stopwords = stopwords.words('english')

stopwords += list(string.punctuation) + ["''", '""', '...', '``', '’', '“', '’', '”', '‘', '‘',"'", '©', 'said',"'s", "also",'one',"n't",'com', '-', '–', '—', '_',"/"]
stopwords = set(stopwords)

def remove_stopwords(text):
    output= [i for i in text if i not in stopwords]
    return output

df_new['clean_text']= df_new['accent_text'].apply(lambda x: x.lower())
print("lower the text, over")
df_new['clean_text']= df_new['clean_text'].apply(lambda x: tokenization(x))
print("tokenization step, over")
df_new['clean_text']= df_new['clean_text'].apply(lambda x:remove_stopwords(x))
print("stop word removal, over")
print("Completed")

df_new['clean_text']

print(df_new['clean_text'][0])

from gensim import corpora
from gensim.models import LdaModel

clean_txt = df_new[df_new['target']==0]['clean_text'].tolist()
# Create Dictionary
id2word = corpora.Dictionary(clean_txt)
# Create Corpus
texts = clean_txt
# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]
# View
print(corpus[:1])

!pip install pyLDAvis

import gensim
import pyLDAvis
# topics (number of topics is 100)
lda_model_100 = gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word,num_topics=100,random_state=100,update_every=0,chunksize=10000,passes=10,alpha='auto',per_word_topics=True)

from sklearn.model_selection import train_test_split
#Binary-Classification - review texts as input and take all the rates that are lower than 4 as 0, and rate greater or equal to 4 as 1 or positive
X = df_new['clean_text'].apply(lambda x: ' '.join(x))
y = df_new['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print('Train:', X_train.shape, y_train.shape, 'Test:', (X_test.shape, y_test.shape))

print('TFIDF Vectorizer...')
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_df=0.9, ngram_range=(1, 1))
# tf-idf matrix with unigram features only
tf_x_train = vectorizer.fit_transform(X_train)
tf_x_test = vectorizer.transform(X_test)

from sklearn.ensemble import RandomForestClassifier

random_forest_classifier = RandomForestClassifier(n_estimators=128)
rfc = random_forest_classifier.fit(tf_x_train, y_train)

y_pred = rfc.predict(tf_x_test)

y_pred

print(accuracy_score(y_pred, y_test))

y_pred.mean()

print("Classification Report is:\n",classification_report(y_test,y_pred))

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(10,7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

fpr, tpr, thresholds = roc_curve(y_test, y_pred)

# Calculate the AUC
roc_auc = auc(fpr, tpr)

# Plotting the ROC Curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='Random Forest Classifier AUC = %0.2f' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()